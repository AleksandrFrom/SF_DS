{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Основы Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle — международная платформа для проведения соревнований по анализу данных на основе реальных задач от различных компаний. Эта площадка может похвастаться самым большим Data Science-комьюнити со всего мира — более 200 тысяч участников, с которыми можно взаимодействовать посредством разных активностей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Площадка Kaggle важна для становления дата-сайентиста: работодатели оценивают активность кандидатов и их достижения на площадке. Для обоих участников рынка труда это удобный инструмент: работодатели имеют возможность увидеть, как кандидат пишет код, излагает мысли, что загружает; а работник может использовать площадку как своё портфолио.\n",
    "\n",
    "Если перейти во вкладку Competitions, можно увидеть список соревнований разных видов. \n",
    "\n",
    "    Соревнование типа Knowledge. За этот вид соревнований для новичков баллы не начисляются. \n",
    "    Соревнования с денежной наградой. \n",
    "    Соревнования, привязанные к определённым курсам, мероприятиям и так далее. \n",
    "\n",
    "Участие в соревнованиях влияет на публичный рейтинг пользователя. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рейтинги составляются по четырём видам активностей: \n",
    "\n",
    "    Competitions (участие в соревнованиях);\n",
    "    Datasets (публикация открытых датасетов);\n",
    "    Notebooks (создание ноутбуков);\n",
    "    Discussions (публикация сообщений на форуме).\n",
    "\n",
    "Пользователи Kaggle могут последовательно достичь следующих пяти разных уровней статуса в любой из категорий: Novice, Contributor, Expert, Master и Grandmaster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗️ Для достижения каждого из этих уровней требуется выполнить определённые условия, которые усложняются с каждым шагом в сторону Grandmaster. Рейтинги Kaggle важны для карьеры любого дата-сайентиста: на текущий момент почти все работодатели обращают пристальное внимание на успехи кандидата на этой площадке. Как правило, работодатели при просмотре профиля соискателя на Kaggle обращают внимание на уровни в категориях Competitions и Notebooks — именно эти категории позволяют оценить специфические навыки дата-сайентиста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Работа с ноутбуками в Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Публикация ноутбуков — один из важных пунктов развития дата-сайентиста на Kaggle. Они позволяют показать сообществу свои идеи относительно конкретного соревнования/датасета и получить обратную связь от сообщества."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Участие в соревновании"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот бейзлайн построен на основе библиотеки, разработанной в Сбере, — LightAutoML (LAMA). В верхней части ноутбука вы увидите ссылку на GitHub-репозиторий этой библиотеки. \n",
    "\n",
    "Библиотека позволяет автоматически строить модели машинного обучения для различных задач, например для бинарной классификации, мультиклассовой классификации и регрессии, на табличных данных (в том числе вещественных и категориальных данных, датах, текстах и даже изображениях). Этот инструмент позволяет сосредоточиться на важных задачах машинного обучения — генерации признаков, выборе метрик, определении целевой переменной и так далее. \n",
    "\n",
    "### LAMA не входит в стандартную поставку образа Python на Kaggle, поэтому прежде всего необходимо установить её. \n",
    "\n",
    "!pip install -U lightautoml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Затем импортируем необходимые библиотеки. \n",
    "\n",
    "# Standard python libraries\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Essential DS libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "# LightAutoML presets, task and report generation\n",
    "from lightautoml.automl.presets.tabular_presets import TabularAutoML\n",
    "from lightautoml.tasks import Task\n",
    "\n",
    "### Далее определяем константы (количество потоков, таймаут, имя целевой колонки, размер валидационной выборки и так далее). \n",
    "\n",
    "N_THREADS = 4\n",
    "N_FOLDS = 5\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "TIMEOUT = 600 # equal to 10 minutes\n",
    "TARGET_NAME = 'final_price'\n",
    "\n",
    "### Для максимальной воспроизводимости решения используем команды: \n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.set_num_threads(N_THREADS)\n",
    "\n",
    "### Указываем путь к папке с данными: \n",
    "\n",
    "INPUT_DIR = '../input/sf-dst-predict-car-price/'\n",
    "\n",
    "### Загружаем тренировочный датасет (в нём будет 15 колонок): \n",
    "\n",
    "train_data = pd.read_csv(INPUT_DIR + 'train_data.csv')\n",
    "print(train_data.shape)\n",
    "train_data.head()\n",
    "\n",
    "### Загружаем тестовый датасет (в нём будет 14 колонок, без колонки с целевой переменной): \n",
    "\n",
    "test_data = pd.read_csv(INPUT_DIR + 'test_data.csv')\n",
    "print(test_data.shape)\n",
    "test_data.head()\n",
    "\n",
    "### Загружаем шаблон сабмита (по количеству строк совпадает с тестовыми данными): \n",
    "\n",
    "submission = pd.read_csv(INPUT_DIR + 'sample_submission.csv')\n",
    "print(submission.shape)\n",
    "submission.head()\n",
    "\n",
    "### Далее в бейзлайне идёт небольшая заготовка для генерации дополнительных признаков. \n",
    "\n",
    "%%time\n",
    "\n",
    "def create_expert_feats(data):\n",
    "    pass\n",
    "\n",
    "create_expert_feats(train_data)\n",
    "create_expert_feats(test_data)\n",
    "\n",
    "### Последний пункт подготовительного этапа нашей работы — разбиение на тренировочную (80 %, tr_data) и валидационную (20 %, te_data) выборки. \n",
    "\n",
    "tr_data, te_data = train_test_split(\n",
    "    train_data, \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f'Data splitted. Parts sizes: tr_data = {tr_data.shape}, te_data = {te_data.shape}')\n",
    "\n",
    "tr_data.head()\n",
    "\n",
    "#### 28 000 строк отойдут в тренировочную выборку, 7 000 — в валидационную. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
